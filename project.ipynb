{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports \n",
    "import cv2\n",
    "import numpy as np\n",
    "import math\n",
    "import glob\n",
    "import time\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from feature_extraction import *\n",
    "from PIL import Image, ImageEnhance\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract features from images\n",
    "is_load = True\n",
    "contours = []\n",
    "labels   = []  # 1 for males  , 0 for females\n",
    "\n",
    "if ( not is_load):\n",
    "    # read male images\n",
    "    flooded_path   = 'dataset_resized/flooded'\n",
    "    flooded_files   = [ f for f in listdir(flooded_path) if isfile(join(flooded_path,f)) ]\n",
    "    for i in tqdm(range(0, len(flooded_files))):\n",
    "        img  = Image.open(join(flooded_path,flooded_files[i]))\n",
    "        img, _  = preprocess_image( img )\n",
    "        contour = get_contour_pixels(img)\n",
    "        contours.append(contour)\n",
    "        labels.append(1)\n",
    "        \n",
    "    ## read female images\n",
    "    non_flooded_path = 'dataset_resized/non-flooded'\n",
    "    non_flooded_files = [ f for f in listdir(non_flooded_path) if isfile(join(non_flooded_path,f)) ]\n",
    "    for i in tqdm(range(0, len(non_flooded_files))):\n",
    "        img  = Image.open( join(non_flooded_path,non_flooded_files[i]))\n",
    "        img, _  = preprocess_image( img )\n",
    "        contour = get_contour_pixels(img)\n",
    "        contours.append(contour)\n",
    "        labels.append(0)\n",
    "\n",
    "    contours = np.asarray(contours , dtype= object)\n",
    "    labels   = np.asarray(labels   , dtype= int )\n",
    "    #save lables to dataset\n",
    "    np.save('labels.npy', labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "hinge_features = []\n",
    "cold_features  = []\n",
    "if(not is_load):\n",
    "    for i in tqdm(range( len(contours)) ):\n",
    "        feature  = get_hinge_features( contours[i] )\n",
    "        hinge_features.append(feature)  \n",
    "    hinge_features = np.asarray(hinge_features , dtype=object)\n",
    "    np.save('features/hinge_features.npy', hinge_features)\n",
    "\n",
    "    for i in tqdm(range( len(contours)) ):\n",
    "        feature  = get_cold_features( contours[i] )\n",
    "        cold_features.append(feature)  \n",
    "    cold_features = np.asarray(cold_features , dtype=object)\n",
    "    np.save('features/cold_features.npy', cold_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(is_load):\n",
    "    hinge_features = np.load('features/hinge_features.npy' , allow_pickle= True)\n",
    "    cold_features = np.load('features/cold_features.npy' ,  allow_pickle= True)\n",
    "    labels = np.load('features/labels.npy' , allow_pickle= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## concate features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(922, 420)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# concatenate features in one flattened array\n",
    "features = np.concatenate( (hinge_features, cold_features) , axis=1)\n",
    "cold_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into training set and test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "# X_train, X_test, y_train, y_test = train_test_split( hinge_features , labels , test_size=0.2 ,  random_state=109) # 80% training and 20% test\n",
    "# X_train, X_test, y_train, y_test = train_test_split( hinge_features , labels , test_size=0.2 ,  random_state=50)\n",
    "X_train, X_test, y_train, y_test = train_test_split( features , labels , test_size=0.2 ,  random_state=175)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Create a based model\n",
    "scaler = StandardScaler()\n",
    "X_train  = scaler.fit_transform(X_train)\n",
    "X_test  = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=20)\n",
    "X_train = pca.fit_transform(X_train)\n",
    "X_test = pca.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8486486486486486\n"
     ]
    }
   ],
   "source": [
    "#Import svm model\n",
    "from sklearn import svm\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "#Create a svm Classifier\n",
    "t0 = time.time()\n",
    "SVM_clf = svm.SVC(kernel= \"linear\" ) # Linear Kernel\n",
    "\n",
    "#Train the model using the training sets\n",
    "SVM_clf.fit(X_train, y_train)\n",
    "\n",
    "#Predict the response for test dataset\n",
    "y_pred = SVM_clf.predict(X_test)\n",
    "t1 = time.time()\n",
    "# clf.score(X_test, y_test)\n",
    "\n",
    "\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
    "# print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random forest tree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 72 candidates, totalling 216 fits\n",
      "{'max_depth': 50, 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 100}\n",
      "0.8548531607765057\n",
      "Accuracy: 0.8918918918918919\n"
     ]
    }
   ],
   "source": [
    "#Import Random Forest Model\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'max_depth': [10, 50, 100],\n",
    "    'min_samples_leaf': [1, 2,5],\n",
    "    'min_samples_split': [ 2, 5],\n",
    "    'n_estimators': [10,20, 100, 200]\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Instantiate the grid search model\n",
    "grid_search = GridSearchCV(estimator = RandomForestClassifier(random_state=42), param_grid = param_grid, cv = 3, n_jobs = -1, verbose = 1)\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(grid_search.best_params_)\n",
    "print(grid_search.best_score_)\n",
    "\n",
    "y_pred = grid_search.predict(X_test)\n",
    "\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
    "# print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 3 candidates, totalling 9 fits\n",
      "Accuracy: 0.8918918918918919\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# params for GradientBoostingClassifier\n",
    "param_grid = {\n",
    "    # 'max_depth': [10, 50, 100],\n",
    "    # 'min_samples_leaf': [1, 2,5],\n",
    "    # 'min_samples_split': [ 2, 5],\n",
    "    'n_estimators': [ 200 , 500 , 1000]\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Instantiate the grid search model\n",
    "grid_search =GridSearchCV(estimator =  GradientBoostingClassifier(random_state=42), param_grid = param_grid, cv = 3, n_jobs = -1, verbose = 1)\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "y_pred = grid_search.predict(X_test)\n",
    "\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
    "# print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# save the model to disk\n",
    "filename = 'rfc.pkl'\n",
    "pickle.dump(grid_search, open(filename, 'wb'))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
