{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports \n",
    "import cv2\n",
    "import numpy as np\n",
    "import math\n",
    "import glob\n",
    "import time\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from feature_extraction import *\n",
    "from PIL import Image, ImageEnhance\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 461/461 [00:17<00:00, 26.02it/s]\n",
      "100%|██████████| 461/461 [00:23<00:00, 19.71it/s]\n"
     ]
    }
   ],
   "source": [
    "# extract features from images\n",
    "is_load = False\n",
    "contours = []\n",
    "labels   = []  # 1 for males  , 0 for females\n",
    "\n",
    "if ( not is_load):\n",
    "    # read male images\n",
    "    male_path   = 'dataset_resized/flooded'\n",
    "    male_files   = [ f for f in listdir(male_path) if isfile(join(male_path,f)) ]\n",
    "    for i in tqdm(range(0, len(male_files))):\n",
    "        img  = Image.open(join(male_path,male_files[i]))\n",
    "        img, _  = preprocess_image( img )\n",
    "        contour = get_contour_pixels(img)\n",
    "        contours.append(contour)\n",
    "        labels.append(1)\n",
    "        \n",
    "    ## read female images\n",
    "    female_path = 'dataset_resized/non-flooded'\n",
    "    female_files = [ f for f in listdir(female_path) if isfile(join(female_path,f)) ]\n",
    "    for i in tqdm(range(0, len(female_files))):\n",
    "        img  = Image.open( join(female_path,female_files[i]))\n",
    "        img, _  = preprocess_image( img )\n",
    "        contour = get_contour_pixels(img)\n",
    "        contours.append(contour)\n",
    "        labels.append(0)\n",
    "\n",
    "    contours = np.asarray(contours , dtype= object)\n",
    "    labels   = np.asarray(labels   , dtype= int )\n",
    "    #save lables to dataset\n",
    "    np.save('labels.npy', labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 922/922 [03:05<00:00,  4.98it/s]\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'dataSet/hinge_features.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m     hinge_features\u001b[39m.\u001b[39mappend(feature)  \n\u001b[1;32m      7\u001b[0m hinge_features \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masarray(hinge_features , dtype\u001b[39m=\u001b[39m\u001b[39mobject\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m np\u001b[39m.\u001b[39;49msave(\u001b[39m'\u001b[39;49m\u001b[39mdataSet/hinge_features.npy\u001b[39;49m\u001b[39m'\u001b[39;49m, hinge_features)\n\u001b[1;32m     10\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m tqdm(\u001b[39mrange\u001b[39m( \u001b[39mlen\u001b[39m(contours)) ):\n\u001b[1;32m     11\u001b[0m     feature  \u001b[39m=\u001b[39m get_cold_features( contours[i] )\n",
      "File \u001b[0;32m<__array_function__ internals>:5\u001b[0m, in \u001b[0;36msave\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/numpy/lib/npyio.py:525\u001b[0m, in \u001b[0;36msave\u001b[0;34m(file, arr, allow_pickle, fix_imports)\u001b[0m\n\u001b[1;32m    523\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m file\u001b[39m.\u001b[39mendswith(\u001b[39m'\u001b[39m\u001b[39m.npy\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m    524\u001b[0m         file \u001b[39m=\u001b[39m file \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m.npy\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m--> 525\u001b[0m     file_ctx \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(file, \u001b[39m\"\u001b[39;49m\u001b[39mwb\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m    527\u001b[0m \u001b[39mwith\u001b[39;00m file_ctx \u001b[39mas\u001b[39;00m fid:\n\u001b[1;32m    528\u001b[0m     arr \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masanyarray(arr)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'dataSet/hinge_features.npy'"
     ]
    }
   ],
   "source": [
    "hinge_features = []\n",
    "cold_features  = []\n",
    "if(not is_load):\n",
    "    for i in tqdm(range( len(contours)) ):\n",
    "        feature  = get_hinge_features( contours[i] )\n",
    "        hinge_features.append(feature)  \n",
    "    hinge_features = np.asarray(hinge_features , dtype=object)\n",
    "    np.save('features/hinge_features.npy', hinge_features)\n",
    "\n",
    "    for i in tqdm(range( len(contours)) ):\n",
    "        feature  = get_cold_features( contours[i] )\n",
    "        cold_features.append(feature)  \n",
    "    cold_features = np.asarray(cold_features , dtype=object)\n",
    "    np.save('features/cold_features.npy', cold_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(is_load):\n",
    "    hinge_features = np.load('features/hinge_features.npy' , allow_pickle= True)\n",
    "    cold_features = np.load('features/cold_features.npy' ,  allow_pickle= True)\n",
    "    labels = np.load('features/labels.npy' , allow_pickle= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## concate features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(363, 420)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# concatenate features in one flattened array\n",
    "features = np.concatenate( (hinge_features, cold_features) , axis=1)\n",
    "cold_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into training set and test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "# X_train, X_test, y_train, y_test = train_test_split( hinge_features , labels , test_size=0.2 ,  random_state=109) # 80% training and 20% test\n",
    "# X_train, X_test, y_train, y_test = train_test_split( hinge_features , labels , test_size=0.2 ,  random_state=50)\n",
    "X_train, X_test, y_train, y_test = train_test_split( hinge_features , labels , test_size=0.2 ,  random_state=175)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Applying PCA function on training\n",
    "# # and testing set of X component\n",
    "# from sklearn.decomposition import PCA\n",
    " \n",
    "# pca = PCA(n_components = 0.95)\n",
    " \n",
    "# # X_train = pca.fit_transform(X_train)\n",
    "# # X_test = pca.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to train the model: in milliseconds:  7.977008819580078\n",
      "Accuracy: 0.7397260273972602\n"
     ]
    }
   ],
   "source": [
    "#Import svm model\n",
    "from sklearn import svm\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "#Create a svm Classifier\n",
    "t0 = time.time()\n",
    "SVM_clf = svm.SVC(kernel= \"linear\" ) # Linear Kernel\n",
    "\n",
    "#Train the model using the training sets\n",
    "SVM_clf.fit(X_train, y_train)\n",
    "\n",
    "#Predict the response for test dataset\n",
    "y_pred = SVM_clf.predict(X_test)\n",
    "t1 = time.time()\n",
    "# clf.score(X_test, y_test)\n",
    "\n",
    "\n",
    "# accuracy & time taken to train the model\n",
    "print(\"Time taken to train the model: in milliseconds: \", (t1-t0)*1000)\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
    "# print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBOOST\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to train the model: in milliseconds:  921.7443466186523 ms\n",
      "Accuracy: 0.9041095890410958\n"
     ]
    }
   ],
   "source": [
    "## make and xgboost model   (pip install xgboost)\n",
    "import xgboost as xgb\n",
    "\n",
    "# create classifier \n",
    "t0 = time.time()\n",
    "XGB_clf = xgb.XGBClassifier(max_depth=3, n_estimators=1000, learning_rate=0.05)\n",
    "# XGB_clf = xgb.XGBClassifier(max_depth=5, n_estimators=5000, learning_rate=0.05)\n",
    "\n",
    "# fit the classifier on the training data\n",
    "XGB_clf.fit(X_train, y_train)\n",
    "\n",
    "# make predictions for the test data\n",
    "y_pred = XGB_clf.predict(X_test)\n",
    "t1 = time.time()\n",
    "\n",
    "# accuracy & time taken to train the model\n",
    "print(\"Time taken to train the model: in milliseconds: \", (t1-t0)*1000, \"ms\")\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
    "# print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random forest tree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to train the model: in milliseconds:  921.7443466186523 ms\n",
      "Accuracy: 0.9041095890410958\n"
     ]
    }
   ],
   "source": [
    "#Import Random Forest Model\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "#Create a Gaussian Classifier  , random_state -> 39 , 60 , 67\n",
    "randomFores_clf=RandomForestClassifier(n_estimators=1000,  random_state = 67)\n",
    "\n",
    "#Train the model using the training sets \n",
    "randomFores_clf.fit(X_train,y_train)\n",
    "\n",
    "y_pred=randomFores_clf.predict(X_test)\n",
    "\n",
    "# accuracy & time taken to train the model\n",
    "print(\"Time taken to train the model: in milliseconds: \", (t1-t0)*1000, \"ms\")\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
    "# print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(classifier , random=None) :\n",
    "    if random is None :\n",
    "        X_train, X_test, y_train, y_test = train_test_split( hinge_features , labels , test_size=0.2)\n",
    "    else:\n",
    "        X_train, X_test, y_train, y_test = train_test_split( hinge_features , labels , test_size=0.2, random_state=random ) # 80% training and 20% test\n",
    "    #Train the model using the training sets\n",
    "    classifier.fit(X_train, y_train)\n",
    "\n",
    "    #Predict the response for test dataset\n",
    "    y_pred = classifier.predict(X_test)\n",
    "\n",
    "    return metrics.accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to train SVM: in miutes 0.008482722441355388 minutes\n",
      "Time taken to train XGBOOST: in miutes 1.6525152762730917 minutes\n",
      "Time taken to train Random forest: in miutes 2.960528488953908 minutes\n"
     ]
    }
   ],
   "source": [
    "# Get avg accuraccy for models ( SVM , XGB , Random Forest)\n",
    "is_get_avg = False \n",
    "if is_get_avg: \n",
    "    svm_list = []\n",
    "    xgb_list = []\n",
    "    rf_list  = []\n",
    "    time_list = []\n",
    "\n",
    "    # train svm model for 100 times\n",
    "    t0 = time.time()\n",
    "    for i in tqdm(range(100, 200)):\n",
    "        svm_list.append( train(SVM_clf ) )\n",
    "    t1 = time.time()\n",
    "    time_list.append( (t1-t0) )\n",
    "\n",
    "    # train xgboost model for 100 times\n",
    "    t0 = time.time()\n",
    "    for i in tqdm(range(100, 200)):\n",
    "        xgb_list.append( train(XGB_clf) )\n",
    "    t1 = time.time()\n",
    "    time_list.append( (t1-t0) )\n",
    "    \n",
    "    # train random forest model for 100 times\n",
    "    t0 = time.time()\n",
    "    for i in tqdm(range(100, 200)):\n",
    "        rf_list.append ( train(randomFores_clf ))\n",
    "    t1 = time.time()\n",
    "    time_list.append( (t1-t0) )\n",
    "\n",
    "print(\"Time taken to train SVM: in miutes\", time_list[0]/60, \"minutes\")\n",
    "print(\"Time taken to train XGBOOST: in miutes\", time_list[1]/60, \"minutes\")\n",
    "print(\"Time taken to train Random forest: in miutes\", time_list[2]/60, \"minutes\")\n",
    "avg_svm = sum(svm_list) / len(svm_list)\n",
    "avg_xgb = sum(xgb_list) / len(xgb_list)\n",
    "avg_rf  = sum(rf_list)  / len(rf_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " avg SVM 0.641232876712329 || max SVM 0.7534246575342466 || min SVM 0.4931506849315068 || argmax 69\n",
      " avg XGB 0.8000000000000002 || max XGB 0.8767123287671232 || min XGB 0.6712328767123288 || argmax 2\n",
      " avg RF  0.7875342465753427  || max RF  0.8904109589041096  || min RF  0.6712328767123288  || argmax 78\n"
     ]
    }
   ],
   "source": [
    "if is_get_avg:\n",
    "    print(f\" avg SVM {avg_svm} || max SVM {max(svm_list)} || min SVM {min(svm_list)} || argmax {np.array(svm_list).argmax()}\")\n",
    "    print(f\" avg XGB {avg_xgb} || max XGB {max(xgb_list)} || min XGB {min(xgb_list)} || argmax {np.array(xgb_list).argmax()}\")\n",
    "    print(f\" avg RF  {avg_rf}  || max RF  {max(rf_list)}  || min RF  {min(rf_list)}  || argmax {np.array(rf_list).argmax() }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_img_hinge( classifier , path):\n",
    "    img  = Image.open(path)\n",
    "    img, _  = preprocess_image(  img)\n",
    "    contour = get_contour_pixels(img)\n",
    "    hinge = get_hinge_features(contour)\n",
    "    tt    = classifier.predict(np.array([hinge]))\n",
    "    if tt == 1 : print(f\"{path} \\t: is Male\") \n",
    "    else       : print(f\"{path} \\t: is Female\") \n",
    "\n",
    "# def test_img_cold( classifier , path):\n",
    "#     img  = Image.open(path)\n",
    "#     img, _  = preprocess_image(  img)\n",
    "#     contour = get_contour_pixels(img)\n",
    "#     cold  = get_cold_features(contour)\n",
    "#     tt    = classifier.predict(np.array([cold]))\n",
    "#     if tt == 1 : print(f\"{path} \\t: is Male\") \n",
    "#     else       : print(f\"{path} \\t: is Female\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test results in notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project Submission/test\\001.jpg \t: is Male\n",
      "Project Submission/test\\002.jpg \t: is Male\n",
      "Project Submission/test\\003.jpg \t: is Male\n",
      "Project Submission/test\\004.jpg \t: is Male\n",
      "Project Submission/test\\005.jpg \t: is Male\n",
      "Project Submission/test\\006.jpg \t: is Male\n",
      "------------------------------------------------\n",
      "Project Submission/test\\001.jpg \t: is Male\n",
      "Project Submission/test\\002.jpg \t: is Male\n",
      "Project Submission/test\\003.jpg \t: is Female\n",
      "Project Submission/test\\004.jpg \t: is Female\n",
      "Project Submission/test\\005.jpg \t: is Female\n",
      "Project Submission/test\\006.jpg \t: is Male\n",
      "------------------------------------------------\n",
      "Project Submission/test\\001.jpg \t: is Male\n",
      "Project Submission/test\\002.jpg \t: is Male\n",
      "Project Submission/test\\003.jpg \t: is Female\n",
      "Project Submission/test\\004.jpg \t: is Female\n",
      "Project Submission/test\\005.jpg \t: is Female\n",
      "Project Submission/test\\006.jpg \t: is Male\n"
     ]
    }
   ],
   "source": [
    "# test_img_hinge( XGB_clf, '/001.jpg')\n",
    "# test_img_hinge( XGB_clf, 'dataSet/Females/F100.jpg')\n",
    "\n",
    "\n",
    "OUTPUT_DIRECTORY   = 'Project Submission/test'\n",
    "test_files   = [ f for f in listdir(OUTPUT_DIRECTORY) if isfile(join(OUTPUT_DIRECTORY,f)) ]\n",
    "for i in range(0, len(test_files)):\n",
    "    img  = Image.open(join(OUTPUT_DIRECTORY,test_files[i]))\n",
    "    test_img_hinge(SVM_clf , join(OUTPUT_DIRECTORY,test_files[i]))\n",
    "print('------------------------------------------------')\n",
    "\n",
    "for i in range(0, len(test_files)):\n",
    "    img  = Image.open(join(OUTPUT_DIRECTORY,test_files[i]))\n",
    "    test_img_hinge(XGB_clf , join(OUTPUT_DIRECTORY,test_files[i]))\n",
    "\n",
    "print('------------------------------------------------')\n",
    "for i in range(0, len(test_files)):\n",
    "    img  = Image.open(join(OUTPUT_DIRECTORY,test_files[i]))\n",
    "    test_img_hinge(randomFores_clf , join(OUTPUT_DIRECTORY,test_files[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:16<00:00,  2.73s/it]\n"
     ]
    }
   ],
   "source": [
    "OUTPUT_DIRECTORY = \"Project Submission/out\" ## * Appended Path of outputs\n",
    "TEST_DIRECTORY   = \"Project Submission/test\" ## * Appended Path of test images\n",
    "answers   = []\n",
    "time_list =[]\n",
    "\n",
    "\n",
    "test_files   = [ f for f in listdir(TEST_DIRECTORY) if isfile(join(TEST_DIRECTORY,f)) ]\n",
    "for i in tqdm(range(0, len(test_files))):\n",
    "    img  = Image.open(join(TEST_DIRECTORY,test_files[i]))\n",
    "    \n",
    "    t0 = time.time()\n",
    "    try: \n",
    "        img, _  = preprocess_image( img )\n",
    "        contour = get_contour_pixels(img)\n",
    "        hinge = get_hinge_features(contour)\n",
    "        ans    = randomFores_clf.predict(np.array([hinge]))\n",
    "        t1 = time.time()\n",
    "\n",
    "        time_list.append( (t1-t0) )\n",
    "        if   ans == 1 :   answers.append(1)\n",
    "        elif ans == 0 : answers.append(0)\n",
    "        else : answers.append(-1)\n",
    "    except:\n",
    "        print(f\"{test_files[i]} \\t: is not a valid image\")\n",
    "        answers.append(-1)\n",
    "        time_list.append( time.time() - t0 )\n",
    "\n",
    "# save the output in txt files \n",
    "with open(join(OUTPUT_DIRECTORY,'results.txt'), 'w') as f:\n",
    "    for i in range(0, len(answers)):\n",
    "        f.write(f\"{answers[i]}\\n\")\n",
    "\n",
    "#write time in txt files \n",
    "with open(join(OUTPUT_DIRECTORY,'times.txt'), 'w') as f:\n",
    "    for i in range(0, len(time_list)):\n",
    "        f.write(f\"{time_list[i]}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.0\t || \t2.726\n"
     ]
    }
   ],
   "source": [
    "OUTPUT_DIRECTORY = \"Project Submission/out\" ## * Appended Path of outputs\n",
    "TEST_DIRECTORY   = \"Project Submission/test\" ## * Appended Path of test images\n",
    "truth = None\n",
    "with open('./ground_truth.txt', 'rb') as gt_file:\n",
    "    truth = [ int(line)  for line in gt_file.readlines()]\n",
    "    truth = np.array(truth)\n",
    "\n",
    "hypothesis = None\n",
    "with open( join(OUTPUT_DIRECTORY,'results.txt'), 'rb') as hypo_file:\n",
    "    hypothesis = [ int(line)  for line in hypo_file.readlines()]\n",
    "    hypothesis = np.array(hypothesis)\n",
    "\n",
    "## ! Account for length mismatch:\n",
    "if len(truth) != len(hypothesis):\n",
    "    truncation_len = min(len(truth), len(hypothesis))\n",
    "    hypothesis = hypothesis[:truncation_len]\n",
    "    truth = truth[:truncation_len]\n",
    "\n",
    "ACCUARICY = np.sum(truth == hypothesis) / len(truth)    \n",
    "\n",
    "## * Time Evaluation:\n",
    "times = None\n",
    "with open(join(OUTPUT_DIRECTORY,'times.txt'), 'rb') as times_file:\n",
    "\n",
    "    times = [float(line) for line in times_file.readlines()]\n",
    "    times = np.array(times)\n",
    "\n",
    "TIME_MEAN = round(np.mean(times), 3)\n",
    "\n",
    "\n",
    "## * Report:\n",
    "print(ACCUARICY*100  , TIME_MEAN , sep = \"\\t || \\t\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
