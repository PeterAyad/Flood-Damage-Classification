{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports \n",
    "import cv2\n",
    "import numpy as np\n",
    "import math\n",
    "import glob\n",
    "import time\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from feature_extraction import *\n",
    "from PIL import Image, ImageEnhance\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 461/461 [00:19<00:00, 23.88it/s]\n",
      "100%|██████████| 461/461 [00:25<00:00, 18.11it/s]\n"
     ]
    }
   ],
   "source": [
    "# extract features from images\n",
    "is_load = False\n",
    "contours = []\n",
    "labels   = []  # 1 for males  , 0 for females\n",
    "\n",
    "if ( not is_load):\n",
    "    # read male images\n",
    "    flooded_path   = 'dataset_resized/flooded'\n",
    "    flooded_files   = [ f for f in listdir(flooded_path) if isfile(join(flooded_path,f)) ]\n",
    "    for i in tqdm(range(0, len(flooded_files))):\n",
    "        img  = Image.open(join(flooded_path,flooded_files[i]))\n",
    "        img, _  = preprocess_image( img )\n",
    "        contour = get_contour_pixels(img)\n",
    "        contours.append(contour)\n",
    "        labels.append(1)\n",
    "        \n",
    "    ## read female images\n",
    "    non_flooded_path = 'dataset_resized/non-flooded'\n",
    "    non_flooded_files = [ f for f in listdir(non_flooded_path) if isfile(join(non_flooded_path,f)) ]\n",
    "    for i in tqdm(range(0, len(non_flooded_files))):\n",
    "        img  = Image.open( join(non_flooded_path,non_flooded_files[i]))\n",
    "        img, _  = preprocess_image( img )\n",
    "        contour = get_contour_pixels(img)\n",
    "        contours.append(contour)\n",
    "        labels.append(0)\n",
    "\n",
    "    contours = np.asarray(contours , dtype= object)\n",
    "    labels   = np.asarray(labels   , dtype= int )\n",
    "    #save lables to dataset\n",
    "    np.save('features/labels.npy', labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 922/922 [03:34<00:00,  4.30it/s]\n",
      "  0%|          | 0/922 [00:00<?, ?it/s]/home/andrew/Desktop/Hand-writting-Classification/Flood-Damage-Classification/feature_extraction.py:121: RuntimeWarning: divide by zero encountered in log10\n",
      "  rhos_log_space = np.log10(rhos)\n",
      " 63%|██████▎   | 585/922 [14:30<08:21,  1.49s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m np\u001b[39m.\u001b[39msave(\u001b[39m'\u001b[39m\u001b[39mfeatures/hinge_features.npy\u001b[39m\u001b[39m'\u001b[39m, hinge_features)\n\u001b[1;32m     10\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m tqdm(\u001b[39mrange\u001b[39m( \u001b[39mlen\u001b[39m(contours)) ):\n\u001b[0;32m---> 11\u001b[0m     feature  \u001b[39m=\u001b[39m get_cold_features( contours[i] )\n\u001b[1;32m     12\u001b[0m     cold_features\u001b[39m.\u001b[39mappend(feature)  \n\u001b[1;32m     13\u001b[0m cold_features \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masarray(cold_features , dtype\u001b[39m=\u001b[39m\u001b[39mobject\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/Hand-writting-Classification/Flood-Damage-Classification/feature_extraction.py:116\u001b[0m, in \u001b[0;36mget_cold_features\u001b[0;34m(contours, approx_poly_factor)\u001b[0m\n\u001b[1;32m    114\u001b[0m point_1s \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([point[\u001b[39m0\u001b[39m] \u001b[39mfor\u001b[39;00m point \u001b[39min\u001b[39;00m cnt])\n\u001b[1;32m    115\u001b[0m x1s, y1s \u001b[39m=\u001b[39m point_1s[:, \u001b[39m0\u001b[39m], point_1s[:, \u001b[39m1\u001b[39m]\n\u001b[0;32m--> 116\u001b[0m point_2s \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([cnt[(i \u001b[39m+\u001b[39m k) \u001b[39m%\u001b[39m n_pixels][\u001b[39m0\u001b[39m] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_pixels)])\n\u001b[1;32m    117\u001b[0m x2s, y2s \u001b[39m=\u001b[39m point_2s[:, \u001b[39m0\u001b[39m], point_2s[:, \u001b[39m1\u001b[39m]\n\u001b[1;32m    119\u001b[0m thetas \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mdegrees(np\u001b[39m.\u001b[39marctan2(y2s \u001b[39m-\u001b[39m y1s, x2s \u001b[39m-\u001b[39m x1s) \u001b[39m+\u001b[39m np\u001b[39m.\u001b[39mpi)\n",
      "File \u001b[0;32m~/Desktop/Hand-writting-Classification/Flood-Damage-Classification/feature_extraction.py:116\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    114\u001b[0m point_1s \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([point[\u001b[39m0\u001b[39m] \u001b[39mfor\u001b[39;00m point \u001b[39min\u001b[39;00m cnt])\n\u001b[1;32m    115\u001b[0m x1s, y1s \u001b[39m=\u001b[39m point_1s[:, \u001b[39m0\u001b[39m], point_1s[:, \u001b[39m1\u001b[39m]\n\u001b[0;32m--> 116\u001b[0m point_2s \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([cnt[(i \u001b[39m+\u001b[39;49m k) \u001b[39m%\u001b[39;49m n_pixels][\u001b[39m0\u001b[39;49m] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_pixels)])\n\u001b[1;32m    117\u001b[0m x2s, y2s \u001b[39m=\u001b[39m point_2s[:, \u001b[39m0\u001b[39m], point_2s[:, \u001b[39m1\u001b[39m]\n\u001b[1;32m    119\u001b[0m thetas \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mdegrees(np\u001b[39m.\u001b[39marctan2(y2s \u001b[39m-\u001b[39m y1s, x2s \u001b[39m-\u001b[39m x1s) \u001b[39m+\u001b[39m np\u001b[39m.\u001b[39mpi)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "hinge_features = []\n",
    "cold_features  = []\n",
    "if(not is_load):\n",
    "    for i in tqdm(range( len(contours)) ):\n",
    "        feature  = get_hinge_features( contours[i] )\n",
    "        hinge_features.append(feature)  \n",
    "    hinge_features = np.asarray(hinge_features , dtype=object)\n",
    "    np.save('features/hinge_features.npy', hinge_features)\n",
    "\n",
    "    for i in tqdm(range( len(contours)) ):\n",
    "        feature  = get_cold_features( contours[i] )\n",
    "        cold_features.append(feature)  \n",
    "    cold_features = np.asarray(cold_features , dtype=object)\n",
    "    np.save('features/cold_features.npy', cold_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'features/hinge_features.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mif\u001b[39;00m(is_load):\n\u001b[0;32m----> 2\u001b[0m     hinge_features \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mload(\u001b[39m'\u001b[39;49m\u001b[39mfeatures/hinge_features.npy\u001b[39;49m\u001b[39m'\u001b[39;49m , allow_pickle\u001b[39m=\u001b[39;49m \u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m      3\u001b[0m     cold_features \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mload(\u001b[39m'\u001b[39m\u001b[39mfeatures/cold_features.npy\u001b[39m\u001b[39m'\u001b[39m ,  allow_pickle\u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m      4\u001b[0m     labels \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mload(\u001b[39m'\u001b[39m\u001b[39mfeatures/labels.npy\u001b[39m\u001b[39m'\u001b[39m , allow_pickle\u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/numpy/lib/npyio.py:417\u001b[0m, in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    415\u001b[0m     own_fid \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    416\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 417\u001b[0m     fid \u001b[39m=\u001b[39m stack\u001b[39m.\u001b[39menter_context(\u001b[39mopen\u001b[39;49m(os_fspath(file), \u001b[39m\"\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m\"\u001b[39;49m))\n\u001b[1;32m    418\u001b[0m     own_fid \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    420\u001b[0m \u001b[39m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'features/hinge_features.npy'"
     ]
    }
   ],
   "source": [
    "if(is_load):\n",
    "    hinge_features = np.load('features/hinge_features.npy' , allow_pickle= True)\n",
    "    cold_features = np.load('features/cold_features.npy' ,  allow_pickle= True)\n",
    "    labels = np.load('features/labels.npy' , allow_pickle= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## concate features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(922, 420)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# concatenate features in one flattened array\n",
    "features = np.concatenate( (hinge_features, cold_features) , axis=1)\n",
    "cold_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into training set and test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "# X_train, X_test, y_train, y_test = train_test_split( hinge_features , labels , test_size=0.2 ,  random_state=109) # 80% training and 20% test\n",
    "# X_train, X_test, y_train, y_test = train_test_split( hinge_features , labels , test_size=0.2 ,  random_state=50)\n",
    "X_train, X_test, y_train, y_test = train_test_split( features , labels , test_size=0.2 ,  random_state=175)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Create a based model\n",
    "scaler = StandardScaler()\n",
    "X_train  = scaler.fit_transform(X_train)\n",
    "X_test  = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=40)\n",
    "X_train = pca.fit_transform(X_train)\n",
    "X_test = pca.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8486486486486486\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.86      0.82        72\n",
      "           1       0.90      0.84      0.87       113\n",
      "\n",
      "    accuracy                           0.85       185\n",
      "   macro avg       0.84      0.85      0.84       185\n",
      "weighted avg       0.85      0.85      0.85       185\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Import svm model\n",
    "from sklearn import svm\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "#Create a svm Classifier\n",
    "t0 = time.time()\n",
    "SVM_clf = svm.SVC(kernel= \"linear\" ) # Linear Kernel\n",
    "\n",
    "#Train the model using the training sets\n",
    "SVM_clf.fit(X_train, y_train)\n",
    "\n",
    "#Predict the response for test dataset\n",
    "y_pred = SVM_clf.predict(X_test)\n",
    "t1 = time.time()\n",
    "# clf.score(X_test, y_test)\n",
    "\n",
    "\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random forest tree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 72 candidates, totalling 216 fits\n",
      "{'max_depth': 50, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 200}\n",
      "0.8629721807422156\n",
      "Accuracy: 0.9027027027027027\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.89      0.88        72\n",
      "           1       0.93      0.91      0.92       113\n",
      "\n",
      "    accuracy                           0.90       185\n",
      "   macro avg       0.90      0.90      0.90       185\n",
      "weighted avg       0.90      0.90      0.90       185\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Import Random Forest Model\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'max_depth': [10, 50, 100],\n",
    "    'min_samples_leaf': [1, 2,5],\n",
    "    'min_samples_split': [ 2, 5],\n",
    "    'n_estimators': [10,20, 100, 200]\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Instantiate the grid search model\n",
    "grid_search = GridSearchCV(estimator = RandomForestClassifier(random_state=42), param_grid = param_grid, cv = 3, n_jobs = -1, verbose = 1)\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(grid_search.best_params_)\n",
    "print(grid_search.best_score_)\n",
    "\n",
    "y_pred = grid_search.predict(X_test)\n",
    "\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 3 candidates, totalling 9 fits\n",
      "Accuracy: 0.8918918918918919\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.86      0.86        72\n",
      "           1       0.91      0.91      0.91       113\n",
      "\n",
      "    accuracy                           0.89       185\n",
      "   macro avg       0.89      0.89      0.89       185\n",
      "weighted avg       0.89      0.89      0.89       185\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# params for GradientBoostingClassifier\n",
    "param_grid = {\n",
    "    # 'max_depth': [10, 50, 100],\n",
    "    # 'min_samples_leaf': [1, 2,5],\n",
    "    # 'min_samples_split': [ 2, 5],\n",
    "    'n_estimators': [ 200 , 500 , 1000]\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Instantiate the grid search model\n",
    "grid_search =GridSearchCV(estimator =  GradientBoostingClassifier(random_state=42), param_grid = param_grid, cv = 3, n_jobs = -1, verbose = 1)\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "y_pred = grid_search.predict(X_test)\n",
    "\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# save the model to disk\n",
    "filename = 'rfc.pkl'\n",
    "pickle.dump(grid_search, open(filename, 'wb'))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
